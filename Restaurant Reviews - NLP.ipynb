{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restaurant Reviews - NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# For text cleaning\n",
    "import re # Regular expression library \n",
    "import nltk # Natural Language Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we will be making a sparse matrix we will need to remove any words not contributing to the\n",
    "review's positive or negative state. \n",
    "\n",
    "\n",
    "We will do this by:\n",
    "1. Removing stop words -  commonly used word (such as 'i','me','my')\n",
    "2. Stemming            - The process of reducing inflected words to their root form (such as loved to love)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gega_PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download the stopwords list from nltk\n",
    "nltk.download('stopwords') \n",
    "# import the downloaded stopwords\n",
    "from nltk.corpus import stopwords\n",
    "# PortStemmer is used for stemming\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "\n",
    "corpus = []\n",
    "for i in range(0, len(dataset)):\n",
    "    # remove everything but a-zA-Z and replace anything removed with a ' '\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i]) \n",
    "    review = review.lower()\n",
    "    review = review.split() # split review into list of words\n",
    "    ps = PorterStemmer()\n",
    "    # remove list of english words not relevant to review. Stopwords contains lists of different languages, must specify English\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] # sets are faster than lists for big sentences\n",
    "    review = ' '.join(review) # go back to a string for each review\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at what this has done to our top 5 reviews:\n",
    "corpus[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we still have names in our reviews like 'rick steve'. These will cause our sparse matrix later on to be needlessly large. \n",
    "\n",
    "We can constrain the maximum features of our matrix and since names aren't as common as other words these can be removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500) # create object of the CountVectorizer class\n",
    "X = cv.fit_transform(corpus).toarray() # tokenisation, need toarray() to crate the matirx\n",
    "y = dataset.iloc[:, 1].values # Get our dependant variable from 'dataset' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: because all our values are either 1 or 0 there is no need for feature scaling here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Now we have our data ready which model do we use?\n",
    "\n",
    "I will try all classification models I know and look at the accuracy of each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(random_state = 0))) \n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier(criterion = 'entropy')))\n",
    "models.append(('RNDFRST', RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(kernel = 'rbf')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix for each and evaluating accuracy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# evaluate each model in turn\n",
    "for name, classifier in models:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    acc = (cm[0][0]+cm[1][1])/200\n",
    "    msg = f'{name} : {str(acc)}'\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes offers the highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Naive Bayes to the Training set\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "# pd.DataFrame(y_pred).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "One thing I noticed earlier was that the stop words included 'not', I feel within reviews this word \n",
    "would hold high importance and therefore high weighting when looking at bad reviews. \n",
    "\n",
    "The second review turned from 'Crust is not good.' to 'crust good'. \n",
    "\n",
    "There would be no differentiation between a review like 'Food is not good' and 'Food is good'. This would 'blur the lines', as it were, in the training stage and I conjecture if this word was included in our corpus we could increase the accuracy of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_amended = []\n",
    "for i in range(0, len(dataset)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i]) \n",
    "    review = review.lower()\n",
    "    review = review.split() \n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in (set(stopwords.words('english')) - set([\"not\"]))] ## keep the word not\n",
    "    review = ' '.join(review)\n",
    "    corpus_amended.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we can see the review now has the word 'not'\n",
    "corpus_amended[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the Bag of Words model with our amended corpus\n",
    "cv = CountVectorizer(max_features = 1500) \n",
    "X = cv.fit_transform(corpus_amended).toarray() \n",
    "y = dataset.iloc[:, 1].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the amended corpus dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re run through the models\n",
    "for name, classifier in models:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    acc = (cm[0][0]+cm[1][1])/200\n",
    "    msg = f'{name} : {str(acc)}'\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare these to our previous results:\n",
    "\n",
    "| Classifier       | Before | After | Difference |\n",
    "|------------------|--------|-------|------------|\n",
    "| LR               | 0.71   | 0.775 | 0.065      |\n",
    "| KNN              | 0.61   | 0.66  | 0.05       |\n",
    "| CART             | 0.725  | 0.745 | 0.02       |\n",
    "| RNDFRST          | 0.72   | 0.725 | 0.005      |\n",
    "| NB               | 0.73   | 0.73  | 0          |\n",
    "| SVM              | 0.485  | 0.485 | 0          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that accross the board we have done better or at least stayed the same.\n",
    "\n",
    "Not only that but the accuracy of Linear Regression has increased so that it is now more accurate than Gaussian Naive Bayes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps:\n",
    "\n",
    "1. I would like to understand a little more the reason why some of the classifiers changed accuracy and why some other didn't after I made my changes. \n",
    "\n",
    "2. A nice feature would be to add a bit of code so that a user can write their own review and the classifier will come back with a prediction of either positive or negative review.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
